"""
:module: eqcutil.core.clusteringtribe.ClusteringTribe
:auth: Nathan T. Stevens
:email: ntsteven@uw.edu
:org: Pacific Northwest Seismic Network
:license: GNU GPLv3
:purpose: This contains a child-class of the :class:`~eqcorrscan.Tribe` class
    and provides extended functionalities for template clustering methods
    as class-methods.
:attribution: This builds on the EQcorrscan project. If you find this class
    useful please be sure to cite both EQcorrscan (e.g., Chamberlain et al., 2017)
    and perhaps this repository as well.

    TODO: provide an option to just save the paths to the source templates
    TODO: change `self.clusters` to `self.index`
    TODO: need an update_id_no method - incorporate into get_subset and remove
"""
import os, logging, tarfile, shutil, pickle, tempfile, glob, fnmatch

from copy import deepcopy
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import squareform
from obspy import read_events, read
from obspy.core.event import Catalog
from eqcorrscan import Tribe, Template

import eqcorrscan.utils.clustering as euc
from eqcorrscan.core.match_filter.helpers import _safemembers, _par_read

from eqcutil.process.clustering import xcorr_cluster_core_process, xcorr_cluster_post_process
from eqcutil.util.pandas import reindex_columns
from eqcutil.viz import eqc_compat

Logger = logging.getLogger(__name__)

class ClusteringTribe(Tribe):
    """An augmentation of the :class:`~eqcorrscan.Tribe` class that
    extends EQcorrscan Tribe clustering and stacking methods as 
    class-methods. This method also includes a method for de-duplicating
    input template names prior to adding them to the **templates** attribute

    Parameters
    ----------
    :param templates: (list of) :class:`~eqcorrscan.Template` objects, defaults to []
    :type templates: list or :class:`~eqcorrscan.Template, optional

    Attributes
    ----------
    :var templates: list of eqcorrscan.Template objects
    :var clusters: pandas.DataFrame of template membership in one or
        more clustering analysis.
    :var cluster_kwargs: dictionary that preserves key-word arguments
        input to the :meth:`~.ClusteringTribe.cluster` method, keyed
        by the **method** selected
    :var dist_mat: :class:`~pandas.DataFrame` object that saves the distance
        matrix generated by cross-correlation clustering between templates.
        Columns and Index values are template names
    """    
    def __init__(self, templates=[]):
        """Initialize a ClusteringTribe object

        :param templates: _description_, defaults to []
        :type templates: list, optional
        """
        super().__init__()
        # If eqc_compat.plant() has not been run, do it now      
        if not hasattr(self,'snuffle'):
            eqc_compat.plant()

        self.clusters = pd.DataFrame(columns=['id_no'])
        self.dist_mat = None
        self.shift_mat = None
        self.cluster_kwargs = {}

        if isinstance(templates, Template):
            templates = [templates]
        elif isinstance(templates, list):
            if all(isinstance(_t, Template) for _t in templates):
                pass
            else:
                raise TypeError
        
        for template in templates:
            self.extend(template)

    def extend(self, other, **options):
        """Extend this ClusteringTribe with more :class:`~eqcorrscan.Template`
        objects.

        Options are fed to :meth:`~.ClusteringTribe.add_template` which provides
        default support for re-naming incoming templates in **other** that have
        duplicate names of templates already in this ClusteringTribe

        :param other: a single or iterable collection of templates
        :type other: eqcorrscan.Template or iterable collection thereof (e.g. eqcorrscan.Tribe)
        """        
        if isinstance(other, Template):
            self.add_template(other,**options)
        elif isinstance(other, Tribe):
            for template in other:
                self.add_template(template,**options)
        elif hasattr(other, '__iter__'):
            if all(isinstance(_e, Template) for _e in other):
                for tempate in other:
                    self.add_template(template,**options)
        else:
            Logger.warning(f'type of other does not conform with ClusteringTribe.extend')

    def __iadd__(self, other):
        self.extend(other)
        return self
    
    def __repr__(self, mode='clusters'):
        rstr = f'ClusteringTribe of {len(self.templates)} templates'
        if mode == 'clusters':
            for col in self.clusters.columns:
                if 'cluster' in col:
                    ngrps = len(self.clusters[col].unique())
                    if col == 'correlation_cluster':
                        istat = f'c_thresh: {self.cluster_kwargs[col]["corr_thresh"]}'
                    elif col in ['space_cluster', 'space_time_cluster']:
                        istat = f'd_thresh: {self.cluster_kwargs[col]["d_thresh"]} km'
                        if col == 'space_time_cluster':
                            istat += f' | t_thresh: {self.cluster_kwargs[col]["t_thresh"]} sec'
                    rstr += f'\n{ngrps} groups for {istat}'
        else:
            raise NotImplementedError
        return rstr

    # Shorthand for clusters
    def get_clusters(self):
        return self.clusters
    
    _c = property(get_clusters)

    def _deduplicate_name(self, other, delimiter='__', start=0):
        if other not in self.clusters.index.values:
            return other
        else:
            if delimiter not in other:
                basename = other
            else:
                basename = other.split('__')[0]
            matches = fnmatch.filter(self.clusters.index.values, basename+'*')
            while f'{basename}{delimiter}{start}' in matches:
                start += 1
            return f'{basename}{delimiter}{start}'

    def add_template(self, other, rename_duplicates=False, **options):
        if isinstance(other, Template):
            if other.name in self.clusters.index.values:
                if rename_duplicates:
                    other.name = self._deduplicate_name(other.name, **options)
                else:
                    raise AttributeError(f'duplicate name {other.name} - aborting add_template')
            self.templates.append(other)
            self.clusters = pd.concat([self.clusters, pd.DataFrame({'id_no':len(self)-1}, index=[other.name])],
                                      axis=0, ignore_index=False)
        else:
            raise TypeError('other must be type eqcorrscan.Template')

    def _get_template_list(self):
        """Create a template_list input for 
        :meth:`~eqcorrscan.utils.clusering.cluster`
        from this ClusteringTribe with the option
        to use template names instead of position
        values.

        :return: _description_
        :rtype: _type_
        """
     
        return [(_t.st, _e) for _e, _t in enumerate(self)]

    def cluster(self, method, **kwargs):
        """Extended wrapper for EQcorrscan Template correlation methods
        In addition to the original options of clustering using catalog
        methods (space_cluster and (space_time_cluster) this method now
        also permits use of :meth:`~eqcorrscan.util.clustering.cluster`
        under the method name `correlation_cluster`. Groups are saved
        to the **clusters** and **cluster_kwargs** attributes, which are
        dictionaries keyed by 

        :param method: _description_
        :type method: _type_
        """
        if len(self) < 2:
            raise AttributeError('insufficient number of templates to cluster') 

        index = []; values = []

        if method in ['space_cluster','space_time_cluster']:
            tribes = Tribe.cluster(self, method, **kwargs)
            for _e, tribe in enumerate(tribes):
                for template in tribe:
                    index.append(template.name)
                    values.append(_e)

        elif method == 'correlation_cluster':
            if 'save_corrmat' in kwargs.keys():
                if not kwargs['save_corrmat']:
                    save_local = False
                    kwargs['save_corrmat'] = True
                else:
                    save_local = True
            else:
                kwargs.update({'save_corrmat': True})
                save_local = False
            groups = euc.cluster(self._get_template_list(), **kwargs)
            if 'save_corrmat' in kwargs.keys():
                self.dist_mat = np.load('dist_mat.npy')
                if not save_local:
                    os.remove('dist_mat.npy')
            for _e, group in enumerate(groups):
                for entry in group:
                    try:
                        index.append(self.templates[entry[1]].name)
                    except:
                        breakpoint()
                    values.append(_e)
        elif method == 'xcc':
            cp_defaults = {'shift_len': 0.,
                        'allow_individual_trace_shifts': True,
                        'replace_nan_distances_with': 'mean',
                        'cores': 'all'}
            pp_defaults = {'corr_thresh': 0.5,
                           'method': 'single',
                           'metric': 'euclidian',
                           'optimal_ordering': False,
                           'criterion':'distance'}
            for _k, _v in kwargs.items():
                if _k in cp_defaults.keys():
                    cp_defaults.update({_k:_v})
                if _k in pp_defaults.keys():
                    pp_defaults.update({_k:_v})
            # Update kwargs for saving purposes
            kwargs.update(pp_defaults)
            kwargs.update(cp_defaults)
            # Run main process
            dist_mat, shift_mat = xcorr_cluster_core_process(
                self._get_template_list(), **cp_defaults
            )
            # Save dist_mat and shift_mat to attributes
            self.dist_mat = dist_mat
            self.shift_mat = shift_mat
            # Get groups from post processing
            groups = xcorr_cluster_post_process(self.dist_mat, **pp_defaults)

            for _e, group in enumerate(groups):
                for entry in group:
                    try:
                        index.append(self.templates[entry[1]].name)
                    except:
                        breakpoint()
                    values.append(_e)
        else:
            raise ValueError(f'method {method} not supported.')

        self.cluster_kwargs.update({method: kwargs})
        if self.clusters is None:
            self.clusters = pd.DataFrame(data=values,columns=[method], index=index)
        elif method not in self.clusters.columns:
            self.clusters = pd.concat([self.clusters, pd.DataFrame({method: values}, index=index)],
                                      axis=1, ignore_index=False)
        else:
            for _e, name in enumerate(index):
                self.clusters.loc[name, method] = values[_e]

    def get_subset(self, names):
        """Get an arbitrary subset of templates based on a list
        of template names. This will also create a subset view
        of the **clusters**, **cluster_kwargs**, and **dist_mat**
        attributes in the output **subset**

        :param names: name or list of names of templates to select
        :type names: str or list-like thereof
        :return:
         - **subset** (:class:`~.ClusteringTribe`) -- subset view
            of templates based on the provided list of template names
        """   
        # Catch single name entry
        if isinstance(names, str):
            names = [names]
        # Catch case where not all names are present
        if not set(names) <= set(self.clusters.index.values):
            raise ValueError('Not all provided names match templates in this ClusterTribe')
        # Proceed with making subset
        subset = self.__class__(templates = [self.select(name) for name in names])
        # Subset the clusters index
        subset.clusters = self.clusters[self.clusters.index.isin(names)]
        subset.cluster_kwargs = self.cluster_kwargs
        # If there is a dist_mat, also subset that
        if self.dist_mat is not None:
            subset.dist_mat = np.full(shape=(len(names), len(names)), fill_value=np.nan)
            for xx,ii in enumerate(subset.clusters.id_no.values):
                for yy,jj in enumerate(subset.clusters.id_no.values):
                    subset.dist_mat[xx,yy] = self.dist_mat[ii,jj]
        # return subset
        return subset
    
    def select_by_attribute(self, column, threshold, evaluator='lt'):
        """placehoder - use this method to select subset events using entries in columns"""
        return

    def select_cluster(self, method, index):
        """Return a subset view of this ClusteringTribe for a specific
        clustering method and cluster index number

        uses the :meth:`~.ClusteringTribe.get_subset` method

        :param method: clustering method name
        :type method: str
        :param index: cluster index  number
        :type index: int
        :return: 
         - **subset** (:class:`~.ClusteringTribe`) -- subset view
         of this ClusteringTribe's contents for the specified method
         and index number
        """        
        if self.clusters is None:
            return 
        elif method not in self.clusters.columns:
            Logger.warning(f'cluster method {method} not yet run on this ClusteringTribe')
        names = self.clusters[self.clusters[method] == index].index.values
        return self.get_subset(names)


    def _get_linkage(self, method='xcc', **kwargs):
        """Perform hierarchical/agglomerative clustering on the templates in this
        Cluster

        :return:
         - **Z** (*numpy.ndarray*) -- linkage matrix
        """        
        # Critical error if correlation clustering has not been run
        if method not in self.clusters.columns:
            Logger.critical(f'Clustering method "{method}" has not been run on this ClusteringTribe')
        # Otherwise grab clustering kwargs as the default kwargs for `linkage`
        else:
            ckw = self.cluster_kwargs[method]
        # Critical error if distance matrix is not pre-calculated
        if self.dist_mat is None:
            Logger.critical('dist_mat not populated')
        # If distance matrix is present, proceed
        else:
            # Get linkage inputs
            rndw = ckw['replace_nan_distances_with']
            for _k in ['method','metric','optimal_ordering']:
                # If one of these kwargs was provided in the method call, use it
                if _k in kwargs.keys():
                    continue
                # Otherwise, if it was specified during correlation_cluster-ing, use that
                elif _k in ckw.keys():
                    kwargs.update({_k: ckw[_k]})
                # If kwarg was unspecified, skip to use defaults from linkage
                else:
                    continue

            dm = self.dist_mat
            # Apply fill
            dist_mat = euc.handle_distmat_nans(dm, rndw)
            # Vectorize
            dist_vect = squareform(dist_mat)
            # Recalculate linkage
            Z = linkage(dist_vect, **kwargs)
            return Z

    def cct_regroup(self, corr_thresh, method='xcc', inplace=False, **kwargs):
        """Regroup cross-correlated templates at a different correlation
        threshold with options to re-define the linkage parameterization
    
        :param corr_thresh: template cross correlation threshold for grouping,
            must be a value in 0 < corr_thresh < 1
        :type corr_thresh: float
        :return: _description_
        :rtype: _type_
        """        
        if method not in self.clusters.columns:
            Logger.critical(f'{method} has not been run on this ClusteringTribe')
        else:
            ckw = self.cluster_kwargs[method]
        if 'precision' in kwargs.keys():
            prec = kwargs.pop('precision')
        else:
            prec = 6
        Z = self._get_linkage(**kwargs)

        if not isinstance(corr_thresh, float):
            Logger.critical(f'corr_thresh must be type float')
        elif not 0 < corr_thresh <= 1:
            Logger.critical(f'corr_thresh must be in (0, 1)')
        
        if corr_thresh == ckw['corr_thresh']:
            if not inplace:
                Logger.info(f'Already grouped for corr_thresh={corr_thresh}')
            return self.clusters[method]
        else:
            # Get new grouping
            indices = euc.fcluster(Z, t= 1 - corr_thresh, criterion='distance')
            output = pd.Series(data=indices, index=self.clusters.index, name=method)
            if inplace:
                self.clusters[method]=indices
                ckw['corr_thresh'] = np.round(corr_thresh,decimals=prec)
            else:
                return output
        
    def dendrogram(self, xlabels='index', corr_thresh=None, scalar=False, method='xcc', **kwargs):
        """Wrapper for :meth:`~scipy.cluster.hierarchy.dendrogram` that uses
        saved attribute values from a running :meth:`~.ClusteringTribe.cluster`
        with method='correlation_cluster' or 'xcc' to produce a dendrogram plot

        :param xlabels: Column names from this :class:`~.CorrelationCluster`
            object's **clusters** attribute to use to populate labels on the
            x-axis of this dendrogram. Defaults to 'index', optional
        :param corr_thresh: correlation threshold to use for defining clusters,
            displayed cutoffs are shown as 1-corr_thresh. Defaults to None
            None uses the correlation_cluster corr_thresh saved in this
            :class:`~.ClusteringTribe` object's **cluster_kwargs** attribute
        :type corr_thresh: None or float, optional
        :param scalar: list of scalars to apply to items in **xlabels**, with
            None inputs skipping a scalar transform on values associated
            with matching elements in **xlabels**, defaults to False
            e.g., if xlabels=['etype','depth'], one might use
            scalar=[None, 1e-3] to convert depths to kilometers, but not
            attempt to multiply string entries in etype. 
            False turns off any assessment of scalars for entries in **xlabels**
        :type scalar: list of float-like values or False, optional
        :param kwargs: key-word argument collector passed to :meth:`~.dendrogram`
        :return: **R** (*dict*) -- output
        :rtype: _type_
        """        
        if method not in self.clusters.columns:
            Logger.critical(f'Clustering method "{method}" has not been run on this ClusteringTribe')
        
        lkwargs = {}
        for _k, _v in kwargs.items():
            if _k in ['method','metric','optimal_ordering']:
                lkwargs.update({_k: kwargs.pop(_k)})

        Z = self._get_linkage(**lkwargs)
        if corr_thresh is None:
            threshold = 1 - self.cluster_kwargs[method]['corr_thresh']
        else:
            threshold = 1 - corr_thresh

        if 'ax' not in kwargs.keys():
            fig = plt.figure()
            ax = fig.add_subplot(111)
            kwargs.update({'ax': ax})
        else:
            ax = kwargs['ax']
            
        xlvalues = None
        if isinstance(xlabels, str):
            if xlabels in self.clusters.columns:
                if scalar:
                    xlvalues = self.clusters[xlabels].values*scalar
                else:
                    xlvalues = self.clusters[xlabels].values
                
            elif xlabels == 'index':
                xlvalues = self.clusters.index.values

        # Add list functionality
        elif isinstance(xlabels, list):
            if all(_e in self.clusters.columns for _e in xlabels):
                xlvalues = []
                for idx, row in self.clusters[xlabels].iterrows():
                    xlv = ''
                    for _e, value in enumerate(row.values):
                        if scalar[_e]:
                            xlv += f'{value*scalar[_e]:.1f}|'
                        else:
                            xlv += f'{value}|'
                    xlv = xlv[:-1]
                    xlvalues.append(xlv)


        else:
            xlvalues = None

        kwargs.update({'color_threshold': threshold})
        if 'distance_sort' not in kwargs.keys():
            kwargs.update({'distance_sort': 'ascending'})

        if 'title' in kwargs.keys():
            title = kwargs.pop('title')
        else:
            title = ''

        R = dendrogram(Z, **kwargs)

        if not xlvalues is None:
            newlabels = []
            for _txt in ax.get_xticklabels():
                ind = int(_txt.get_text())
                newlabel = xlvalues[ind]
                newlabels.append(newlabel)
            ax.set_xticklabels(newlabels)
            if isinstance(xlabels, str):
                ax.set_xlabel(xlabels)
            elif isinstance(xlabels, list):
                ax.set_xlabel('|'.join(xlabels))
        else:
            ax.set_xlabel('Entry Number')
        ax.set_ylabel('Linkage Distance\n[1 - corr]')
        ckw = self.cluster_kwargs[method]
        title += f'Fill Value: {ckw["replace_nan_distances_with"]} | '
        title += f'Corr Thresh: {1 - threshold:.3f} | Shift Length: {ckw["shift_len"]} sec'
        title += f' | Individual Shifts: {ckw["allow_individual_trace_shifts"]}'
        ax.set_title(title)
        return R





    def write(self, filename, compress=True, catalog_format="QUAKEML"):
        """
        Write the clusteringtribe to a file using tar archive formatting.

        :type filename: str
        :param filename:
            Filename to write to, if it exists it will be appended to.
        :type compress: bool
        :param compress:
            Whether to compress the tar archive or not, if False then will
            just be files in a folder.
        :type catalog_format: str
        :param catalog_format:
            What format to write the detection-catalog with. Only Nordic,
            SC3ML, QUAKEML are supported. Note that not all information is
            written for all formats (QUAKEML is the most complete, but is
            slow for IO).

        .. rubric:: Example

        >>> tribe = ClusteringTribe(templates=[Template(name='c', st=read())])
        >>> tribe.write('test_tribe')
        Tribe of 1 templates
        >>> tribe.write(
        ...    "this_wont_work.bob",
        ...    catalog_format="BOB") # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        TypeError: BOB is not supported
        """
        from eqcorrscan.core.match_filter import CAT_EXT_MAP

        if catalog_format not in CAT_EXT_MAP.keys():
            raise TypeError("{0} is not supported".format(catalog_format))
        dirname, ext = os.path.splitext(filename)

        # Make directory if it doesn't exist
        if not os.path.isdir(dirname):
            os.makedirs(dirname)
        self._par_write(dirname)

        # Compose tribe catalog
        tribe_cat = Catalog()
        for t in self.templates:
            if t.event is not None:
                # Check that the name in the comment matches the template name
                for comment in t.event.comments:
                    if not comment.text:
                        comment.text = "eqcorrscan_template_{0}".format(t.name)
                    elif comment.text.startswith("eqcorrscan_template_"):
                        comment.text = "eqcorrscan_template_{0}".format(t.name)
                tribe_cat.append(t.event)

        # Write catalog to disk
        if len(tribe_cat) > 0:
            tribe_cat.write(
                os.path.join(dirname, 'tribe_cat.{0}'.format(
                    CAT_EXT_MAP[catalog_format])), format=catalog_format)
            
        # Write template streams to disk
        for template in self.templates:
            template.st.write(
                os.path.join(dirname, '{0}.ms'.format(template.name)),
                format='MSEED')
        # ADDED BY NTS - write clustering summary to disk
        self.clusters.to_csv(os.path.join(dirname,'clusters.csv'), header=True, index=True)

        # Write clustering kwargs to disk
        for _k, _v in self.cluster_kwargs.items():
            with open(os.path.join(dirname, f'{_k}_kwargs.csv'), 'w') as file:
                for _l, _w in _v.items():
                    if isinstance(_w, str):
                        file.write(f'{_l},{_w}\n')
                    else:
                        file.write(f'{_l},{repr(_w)}\n')
        # Write dist_mat to disk
        if self.dist_mat is not None:
            np.save(os.path.join(dirname,'dist_mat.npy'), np.array(self.dist_mat))
        # Write shift_mat to disk
        if self.shift_mat is not None:
            np.save(os.path.join(dirname,'shift_mat.npy'), np.array(self.shift_mat))
        # Run compression if specified
        if compress:
            if not filename.endswith(".tgz"):
                Logger.info("Appending '.tgz' to filename.")
                filename += ".tgz"
            with tarfile.open(filename, "w:gz") as tar:
                tar.add(dirname, arcname=os.path.basename(dirname))
            shutil.rmtree(dirname)
        return self

    def read(self, filename):
        """
        Read a clustertribe of templates from a tar formatted file.

        :type filename: str
        :param filename: File to read templates from.

        .. rubric:: Example

        >>> tribe = Tribe(templates=[Template(name='c', st=read())])
        >>> tribe.write('test_tribe')
        Tribe of 1 templates
        >>> tribe_back = Tribe().read('test_tribe.tgz')
        >>> tribe_back == tribe
        True
        >>> # This can also read pickled templates
        >>> import pickle
        >>> with open("test_tribe.pkl", "wb") as f:
        ...    pickle.dump(tribe, f)
        >>> tribe_back = Tribe().read("test_tribe.pkl")
        >>> tribe_back == tribe
        True
        """
        if filename.endswith(".pkl"):
            with open(filename, "rb") as f:
                self.__iadd__(pickle.load(f))
            return self
        with tarfile.open(filename, "r:*") as arc:
            temp_dir = tempfile.mkdtemp()
            arc.extractall(path=temp_dir, members=_safemembers(arc))
            tribe_dir = glob.glob(temp_dir + os.sep + '*')[0]
            self._read_from_folder(dirname=tribe_dir)
        shutil.rmtree(temp_dir)
        # Assign unique ids
        # self.__unique_ids()
        return self

    def _read_from_folder(self, dirname):
        """
        Internal folder reader.

        :type dirname: str
        :param dirname: Folder to read from.
        """
        templates = _par_read(dirname=dirname, compressed=False)
        # Template Waveform Files
        t_files = glob.glob(dirname + os.sep + '*.ms')
        # Catalog Files
        tribe_cat_file = glob.glob(os.path.join(dirname, "tribe_cat.*"))
        # NEW - cluster summary file
        cluster_file = glob.glob(os.path.join(dirname,'clusters.csv'))
        # NEW - clustering kwargs file
        cluster_kwarg_files = glob.glob(os.path.join(dirname,'*_kwargs.csv'))
        # NEW - distance matrix file
        dist_mat_file = glob.glob(os.path.join(dirname, 'dist_mat.npy'))
        # NEW - shift matrix file
        shift_mat_file = glob.glob(os.path.join(dirname, 'shift_mat.npy'))
        # Load catalog if it is present
        if len(tribe_cat_file) != 0:
            tribe_cat = read_events(tribe_cat_file[0])
        else:
            tribe_cat = Catalog()

        # Load templates with new names
        previous_template_names = [t.name for t in self.templates]
        for template in templates:
            if template.name in previous_template_names:
                # Don't read in for templates that we already have.
                continue
            for event in tribe_cat:
                for comment in event.comments:
                    if comment.text == 'eqcorrscan_template_' + template.name:
                        template.event = event
            t_file = [t for t in t_files
                      if t.split(os.sep)[-1] == template.name + '.ms']
            if len(t_file) == 0:
                Logger.error('No waveform for template: ' + template.name)
                continue
            elif len(t_file) > 1:
                Logger.warning('Multiple waveforms found, using: ' + t_file[0])
            template.st = read(t_file[0])
        # Remove templates that do not have streams
        templates = [t for t in templates if t.st is not None]
        self.templates.extend(templates)

        # Re-constitute groups
        if len(cluster_file) != 0:
            clusters = pd.read_csv(cluster_file[0], index_col=[0])
        else:
            clusters = pd.DataFrame()
        # Remove lines that don't match loaded templates
        if len(clusters) != 0:
            clusters = clusters[clusters.index.isin([_t.name for _t in templates])]

            if set(clusters.index.values) == {_t.name for _t in templates}:
                if self.clusters is None:
                    self.clusters = clusters
                else:
                    self.clusters = pd.concat([self.clusters, clusters], axis=0, ignore_index=False)
            else:
                Logger.error('cluster_group file names loaded do not match template names loaded')

        # Reconstitute processing information
        for ckf in cluster_kwarg_files:
            path, name = os.path.split(ckf)
            name, ext = os.path.splitext(name)
            ctype = name[:-7]
            self.cluster_kwargs.update({ctype: {}})
            df = pd.read_csv(ckf, index_col=[0], header=None)
            for _k, _r in df.iterrows():
                _r = _r.values[0]
                if _r == 'True':
                    _r = True
                elif _r == 'False':
                    _r = False
                else:
                    try:
                        float(_r)
                        _r = float(_r)
                    except ValueError:
                        pass
                self.cluster_kwargs[ctype].update({_k: _r})

                # try:
                #     self.cluster_kwargs[ctype].update({_k: float(_r.values[0])})
                # except ValueError:
                #     self.cluster_kwargs[ctype].update({_k: _r.values[0]})
                
        # Load dist_mat
        if len(dist_mat_file) == 1:
            dist_mat = np.load(dist_mat_file[0])
            self.dist_mat = dist_mat
        if len(shift_mat_file) == 1:
            shift_mat = np.load(shift_mat_file[0])
            self.shift_mat = shift_mat
        return

    def select_template_traces(self, remove_empty_templates=True, **kwargs):
        """Use the :meth:`~obspy.core.stream.Stream.select` to subsample the
        streams attached to each :class:`~eqcorrscan.Template` in this :class:`~.ClusteringTribe`

        Provides the option to get rid of empty templates

        NOTE: This method applies in-place changes to template and stream
        objects in this ClusteringTribe. If you want to save your data, use
        the :meth:`~.ClusteringTribe.copy` method to create a duplicate
        before running this method.

        :param remove_empty_templates: should empty templates be removed from this Tribe?
            Defaults to True
        :type remove_empty_templates: bool, optional
        :param kwargs: key-word argument collector that passes kwargs to 
            :meth:`~obspy.core.stream.Stream.select`
        """        
        # Iterate across templates
        for template in self:
            # Use obspy.core.stream.Stream.select to subset trace
            template.st = template.st.select(**kwargs)
            # Assess if template waveforms is empty & if we want to remove empties
            if len(template.st) == 0 and remove_empty_templates:
                # If so, remove using eqcorrscan.Tribe.remove
                self.remove(template)


    def copy(self):
        return deepcopy(self)


    def remove(self, template):
        """Remove a specified template from this ClusteringTribe

        removes both the template and it's entry from the **clusters**

        :param template: _description_
        :type template: _type_

        DEBUG: Need to re-index id_no before ending
        """        
        if template in self.templates:
            # remove the template entry from self.clusters
            self.clusters.drop(labels=template.name, inplace=True)
            # remove the template
            Tribe.remove(self, template)

    def reindex_columns(self, group='correlation_cluster', ascending=False):
        """Reindex a specified group by decending (or ascending)
        number of members

        :param group: name of the group to re-index. Defaults to 'correlation_cluster'
        :type group: str, optional
        :param ascending: Should group number get bigger with more members? Defaults to False
        :type ascending: bool, optional
        """        
        return reindex_columns(self.clusters, group, ascending=ascending, inplace=True)
                
        


    # # def get_summary(self):
    # #     """Return a summary of the clustering membership of
    # #     each template in this ClusteringTribe. Columns are
    # #     clustering method names, Index values are template names
    # #     integer values are the 

    # #     :return: _description_
    # #     :rtype: _type_
    # #     """        
    # #     index = [_t.name for _t in self]
    # #     columns = list(self.clusters.key())
    # #     if len(columns) > 0:
    # #         data = np.full(shape=(len(index), len(columns)), fill_value=np.nan)
    # #     df = pd.DataFrame(data=data, index=index, columns=columns)
    # #     # Iterate over cluster type
    # #     for ctype, cgroups in self.clusters.items():
    # #         # Iterate over subtribe number and subtribe
    # #         for _stn, tribe in cgroups.items():
    # #             # Iterate over template in tribe
    # #             for template in tribe:
    # #                 # Write subgroup number to name,cluster_type position
    # #                 df.loc[template.name, ctype] = _stn
    # #     return df

    # # summary = property(get_summary)


    
    # template_list = property(get_template_list)

    # def extend(self, other, **options):
    #     Tribe.extend(self, other, **options)
    #     new_lines = 



    


    # def corr_cluster(self, savedir='cluster_results', show=False, corr_thresh=0.3,
    #             shift_len=1., allow_individual_trace_shifts=False, dist_nan_fill=None,
    #            cores='all', method='linear', metric='euclidian',
    #            optimal_ordering=False, save_subtribes=False):
    #     """Wraps the :meth:`~eqcorrscan.util.clustering.cluster` method and 
    #     added saving and formatting functionalities provided by the eqcorrscan_utils
    #     package. 

    #     :param savedir: where clustering results are saved, defaults to 'cluster_results'
    #     :type savedir: str, optional
    #     :param show: should the sub-call of `cluster` display the groups as a dendrogram? Defaults to False
    #     :type show: bool, optional
    #     :param corr_thresh: correlation threshold for grouping, defaults to 0.3
    #     :type corr_thresh: float, optional
    #     :param shift_len: number of seconds templates are allowed to be shifted
    #         durring cross correlations, defaults to 1.
    #     :type shift_len: float, optional
    #     :param allow_individual_trace_shifts: allow each trace in a template to be
    #         shifted during cross correlations? Defaults to False
    #     :type allow_individual_trace_shifts: bool, optional
    #     :param dist_nan_fill: fill value for NaN entries in the distance matrix
    #         calculated by `cluster` (its replace_nan_distances_with arg),
    #         defaults to None.
    #         Supported Values:
    #          - 'mean'
    #          - 'min'
    #          - [0, 1]
    #          also see :meth:`~eqcorrscan.utils.clustering.cluster`
    #     :type dist_nan_fill: str, float, or NoneType, optional
    #     :param cores: number of cores to use for `cluster`, defaults to 'all'
    #     :type cores: str, optional
    #     :param method: linkage method, defaults to 'linear'
    #         also see :meth:`~scipy.clustering.hierarchy.linkage
    #     :type method: str, optional
    #     :param metric: linkage metric, defaults to 'euclidian'
    #     :type metric: str, optional
    #     :param optimal_ordering: linkage optimal_ordering, defaults to False
    #     :type optimal_ordering: bool, optional
    #     :param save_subtribes: should subtribes be saved in savedir?, defaults to False
    #     :type save_subtribes: bool, optional
    #     """



    #     # Create Template List
    #     template_list = [(_t.st, _e) for _e, _t in enumerate(self)]
    #     # Run Clustering
    #     groups = euc.cluster(
    #         template_list,
    #         show=show,
    #         corr_thresh=corr_thresh,
    #         shift_len=shift_len,
    #         allow_individual_trace_shifts=allow_individual_trace_shifts,
    #         replace_nan_distances_with=dist_nan_fill,
    #         cores=cores,
    #         method=method,
    #         metric=metric,
    #         optimal_ordering=optimal_ordering)
    #     # Populate/re-initialize subtribes
    #     self.clusters = {}
    #     if savedir:
    #         subtribes = save_cluster_results(self, groups, savedir=savedir, save_subtribes=save_subtribes)
    #         for _k, _v in subtribes.items():
    #             self.clusters.update({_k, self.__class__(templates=_v)})
    #     else:
    #         for _e, group in groups:
    #             self.clusters.update({_e: self.__class__()})
    #             for entry in group:
    #                 self.clusters[_e] += self.templates[entry[1]]
    #     # Load the distance matrix into this 
    #     self.dist_mat = np.load(Path().cwd() / 'dist_mat.npy')
    #     self.clustering_threshold=corr_thresh
    #     self.dist_nan_fill = dist_nan_fill
    #     self.savedir = savedir
    

    # def write(self, savedir):
    #     if self.groups is None:
    #         Tribe.write(self, savedir, 'templates.tgz')
    #     gfile = os.path.join(savedir, 'groups.csv')
        


    # def load(self, loaddir):
    #     gfile = os.path.join(loaddir, 'groups.csv')
    #     pfile = os.path.join()
    #     if not os.path.isfile(os.path.join(loaddir,'groups.csv')):
    #         Logger.critical(f'could not find groups.csv in {loaddir}')
    #     else:
    #         with open()
